#!/usr/bin/env python3
"""
ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç®¡ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import time
import logging
from event_scraper import EventScraper

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python manage_scraper.py start     # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹")
        print("  python manage_scraper.py run       # 1å›å®Ÿè¡Œ")
        print("  python manage_scraper.py test      # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        print("  python manage_scraper.py stats     # çµ±è¨ˆè¡¨ç¤º")
        return
    
    command = sys.argv[1]
    scraper = EventScraper()
    
    if command == "start":
        print("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™...")
        scraper.schedule_scraping()
        scraper.run_scheduler()
        
    elif command == "run":
        print("ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’1å›å®Ÿè¡Œã—ã¾ã™...")
        start_time = time.time()
        scraper.daily_scraping()
        duration = time.time() - start_time
        print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº† (å®Ÿè¡Œæ™‚é–“: {duration:.2f}ç§’)")
        
    elif command == "test":
        print("ğŸ§ª ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™...")
        # æœ€åˆã®2éƒ½å¸‚ã®ã¿ã§ãƒ†ã‚¹ãƒˆ
        test_sources = dict(list(scraper.sources.items())[:2])
        
        for source_id, source_info in test_sources.items():
            print(f"ãƒ†ã‚¹ãƒˆ: {source_info['name']}")
            events = scraper.scrape_city_website(source_id, source_info, limit=3)
            print(f"  å–å¾—ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {len(events)}")
            for event in events:
                print(f"    - {event.get('title', 'No title')}")
        
    elif command == "stats":
        print("ğŸ“ˆ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ:")
        print(f"  ç·ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {scraper.stats['total_events']}")
        print(f"  æ–°è¦ã‚¤ãƒ™ãƒ³ãƒˆ: {scraper.stats['new_events']}")
        print(f"  æ›´æ–°ã‚¤ãƒ™ãƒ³ãƒˆ: {scraper.stats['updated_events']}")
        print(f"  ã‚¨ãƒ©ãƒ¼æ•°: {scraper.stats['errors']}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å®Ÿéš›ã®çµ±è¨ˆã‚’å–å¾—
        conn = scraper.db_path
        import sqlite3
        conn = sqlite3.connect(scraper.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM events WHERE is_active = 1")
        active_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM events WHERE created_at >= date('now', '-7 days')")
        recent_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT source_city, COUNT(*) FROM events WHERE is_active = 1 GROUP BY source_city")
        city_stats = cursor.fetchall()
        
        conn.close()
        
        print(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¤ãƒ™ãƒ³ãƒˆ: {active_count}")
        print(f"  éå»7æ—¥é–“ã®æ–°è¦: {recent_count}")
        print("  éƒ½å¸‚åˆ¥çµ±è¨ˆ:")
        for city, count in city_stats:
            print(f"    {city}: {count}ä»¶")
    
    else:
        print(f"âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {command}")

if __name__ == "__main__":
    main() 